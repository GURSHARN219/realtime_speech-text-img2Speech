Here's your content converted into a proper `README.md` format, ready to be used in your GitHub repository:

---

````markdown
# ğŸ™ï¸ Voice Chatbot "Sophia" - College Project

Sophia is a real-time **voice chatbot** designed for interactive conversations. It uses optimized local models for STT, TTS, and LLM response generation â€” all running efficiently on CPU/GPU.

---

## ğŸ”§ 1. Overview

Sophia can:
- ğŸ¤ Listen to human speech using **Voice Activity Detection (VAD)**
- ğŸ“ Transcribe speech to text using **Faster-Whisper**
- ğŸ’¬ Generate responses via **Gemma3:4b** (local LLM)
- ğŸ—£ï¸ Convert text replies into natural-sounding voice using **Kokoro TTS**
- ğŸ’» Runs **fully offline** on your local system (GPU/CPU)

---

## ğŸ§  2. System Architecture

```mermaid
graph TD
A[ğŸ™ï¸ Microphone] --> B{ğŸ” VAD Filter}
B -- Speech Detected --> C[ğŸ“ Whisper STT]
B -- Silence --> E[ğŸ” Re-Listen]
C --> D[ğŸŒ Language Detection]
D --> F[ğŸ§  Gemma LLM]
F --> G[ğŸ—£ï¸ Kokoro TTS]
G --> H[ğŸ”Š Speaker]
````

---

## âš™ï¸ 3. Key Components

### 3.1 ğŸ”‡ Voice Activity Detection (VAD)

* **Purpose**: Filters background noise; captures only actual speech.
* **Code Sample**:

```python
vad = webrtcvad.Vad(2)  # Aggressiveness level (0â€“3)
is_speech = vad.is_speech(audio_frame, SAMPLING_RATE)
```

### 3.2 ğŸ“ Speech-to-Text (Whisper)

* **Model**: `faster-whisper-large-v3-turbo-ct2`
* **Code Sample**:

```python
segments, _ = stt_model.transcribe(audio.wav, beam_size=5)
text = " ".join(segment.text for segment in segments)
```

### 3.3 ğŸŒ Language Detection

* **Library**: `langdetect`
* **Code Sample**:

```python
langs = detect_langs(text)  # e.g., [en:0.998, fr:0.002]
primary_lang = langs[0].lang
```

### 3.4 ğŸ§  Response Generation (Gemma LLM)

* **Prompt Injection**:

```python
full_prompt = f"{character_personality}\n\n{user_input}"
```

* **API Call**:

```python
requests.post("http://localhost:11434/api/generate", 
              json={"model": "gemma3:4b", "prompt": full_prompt})
```

### 3.5 ğŸ—£ï¸ Text-to-Speech (Kokoro)

* **Pipeline**:

```python
pipeline = KPipeline(lang_code="a", repo_id="hexgrad/Kokoro-82M")
generator = pipeline(response, voice="af_heart")
```

* **Playback**: Uses `simpleaudio` for low-latency audio output.

---

## ğŸ” 4. Workflow

1. ğŸ§ Microphone â†’ VAD â†’ Collect valid audio frames
2. ğŸ“ Save â†’ Whisper STT â†’ Detect language
3. ğŸ’¬ Inject personality â†’ Query LLM (Gemma)
4. ğŸ—£ï¸ Generate TTS â†’ Play output

---

## ğŸ§© 5. Setup Guide

### ğŸ“¦ Dependencies

```bash
pip install numpy torch sounddevice webrtcvad faster-whisper langdetect simpleaudio
git clone https://github.com/hexgrad/Kokoro-82M  # For Kokoro TTS
```

### ğŸ–¥ï¸ Hardware

* GPU Recommended (for Whisper and Kokoro)
* Microphone + Speakers

### â–¶ï¸ Run the Chatbot

1. Start Ollama LLM server:

```bash
ollama serve  # Gemma runs at http://localhost:11434
```

2. Run the chatbot:

```bash
python sophia_chatbot.py
```

---

## âš¡ 6. Performance Metrics

| Component   | Latency (GPU) | Latency (CPU) |
| ----------- | ------------- | ------------- |
| VAD         | < 10ms        | < 10ms        |
| Whisper STT | \~1.2s        | \~4.5s        |
| Gemma LLM   | \~0.8s        | \~3.1s        |
| Kokoro TTS  | \~0.5s        | \~1.8s        |

---

## ğŸ› ï¸ 7. Customization

* **Change Personality**: Edit the `character_personality` string
* **Swap Models**:

  * Use smaller `faster-whisper` variants (e.g., `tiny.en`)
  * Change `OLLAMA_MODEL` to `llama3`, `mistral`, etc.
* **Voice Options**: Use `af_heart`, `jp_005`, or any supported by Kokoro

---

## ğŸ§ª 8. Troubleshooting

| Issue              | Solution                                                  |
| ------------------ | --------------------------------------------------------- |
| No speech detected | Use `Vad(1)` instead of `Vad(2)`                          |
| Ollama error       | Ensure `gemma3:4b` is downloaded: `ollama pull gemma3:4b` |
| CUDA OOM           | Use smaller Whisper model or set `compute_type="int8"`    |

---

## ğŸš€ 9. Future Enhancements

* Wake-word detection ("Hey Sophia")
* Add memory (contextual conversation)
* Multilingual support (output speech in input language)

---

### ğŸ‘¨â€ğŸ“ Project by: **Gursharn Singh**

**Course**: BTech CSE (AIML)
**GitHub**: [GURSHARN219/realtime\_speech-text-img2Speech](https://github.com/GURSHARN219/realtime_speech-text-img2Speech.git)

```

---

Let me know if you want it saved as a `.md` file or want to embed project screenshots, badges, or demo GIFs for a more polished GitHub presentation.
```
